/**
 * Super Agent Chat - Tool Calling ì§€ì› ì±„íŒ… ì‹œìŠ¤í…œ
 * Cursor/Claude Codeê¸‰ ì—ì´ì „íŠ¸ ê¸°ëŠ¥
 */

import { ChatOpenAI } from '@langchain/openai'
import { ChatOllama } from '@langchain/ollama'
import { ChatGoogleGenerativeAI } from '@langchain/google-genai'
import { HumanMessage, SystemMessage, AIMessage, ToolMessage } from '@langchain/core/messages'
import { getSuperAgentTools, ToolAction } from './super-agent-tools'
import { getDefaultModel, LLMProvider } from '@/lib/llm/client'
import {
  buildDynamicAgentSystemPrompt,
  AGENT_ROLE_PROMPTS,
} from '@/lib/agent/shared-prompts'

// ============================================
// íƒ€ì… ì •ì˜
// ============================================
export interface SuperAgentMessage {
  role: 'user' | 'assistant' | 'system' | 'tool'
  content: string
  toolCalls?: ToolCallInfo[]
  toolCallId?: string
}

export interface ToolCallInfo {
  id: string
  name: string
  arguments: Record<string, unknown>
}

export interface SuperAgentResponse {
  message: string
  actions: ToolAction[]  // í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‹¤í–‰í•  ì•¡ì…˜ë“¤
  toolsUsed: string[]
  thinking?: string
}

interface AgentConfig {
  id: string
  name: string
  description?: string
  capabilities?: string[]
  llm_provider?: string | null
  model?: string | null
  temperature?: number | null
  system_prompt?: string | null
  identity?: any
  apiKey?: string | null
}

interface ChatContext {
  projectPath?: string | null
  userName?: string
  userRole?: string
  workContext?: string
  files?: Array<{ path: string; content?: string }>
}

// ============================================
// LLM ìƒì„±
// ============================================
function createLLM(provider: LLMProvider, model: string, apiKey?: string, temperature = 0.7) {
  switch (provider) {
    case 'openai':
      return new ChatOpenAI({
        model,
        temperature,
        apiKey: apiKey || process.env.OPENAI_API_KEY,
      })

    case 'grok':
      return new ChatOpenAI({
        model,
        temperature,
        apiKey: apiKey || process.env.XAI_API_KEY,
        configuration: {
          baseURL: 'https://api.x.ai/v1',
        },
      })

    case 'gemini':
      return new ChatGoogleGenerativeAI({
        model,
        temperature,
        apiKey: apiKey || process.env.GOOGLE_API_KEY,
      })

    case 'qwen':
      return new ChatOpenAI({
        model,
        temperature,
        apiKey: apiKey || process.env.DASHSCOPE_API_KEY,
        configuration: {
          baseURL: 'https://dashscope-intl.aliyuncs.com/compatible-mode/v1',
        },
      })

    case 'ollama':
      return new ChatOllama({
        model,
        temperature,
        baseUrl: 'http://localhost:11434',
      })

    default:
      return new ChatOllama({
        model: 'qwen2.5:3b',
        temperature: 0.7,
      })
  }
}

// ============================================
// ì—­í•  ì¶”ì¶œ
// ============================================
function getAgentRole(capabilities: string[]): string {
  if (capabilities.includes('development') || capabilities.includes('coding')) return 'developer'
  if (capabilities.includes('design') || capabilities.includes('ui')) return 'designer'
  if (capabilities.includes('marketing') || capabilities.includes('growth')) return 'marketer'
  if (capabilities.includes('analytics') || capabilities.includes('data')) return 'analyst'
  if (capabilities.includes('management') || capabilities.includes('planning')) return 'pm'
  return 'default'
}

// ============================================
// ìŠˆí¼ ì—ì´ì „íŠ¸ ì±„íŒ… ì‘ë‹µ ìƒì„± (Tool Calling ì§€ì›)
// ============================================
export async function generateSuperAgentResponse(
  agent: AgentConfig,
  userMessage: string,
  chatHistory: SuperAgentMessage[] = [],
  context?: ChatContext
): Promise<SuperAgentResponse> {
  // LLM ì„¤ì •
  const provider = (agent.llm_provider || 'grok') as LLMProvider
  const model = agent.model || getDefaultModel(provider)
  const temperature = agent.temperature ?? 0.7

  console.log(`[SuperAgent] ${agent.name} using ${provider}/${model} with tool calling`)

  // LLM ìƒì„±
  const llm = createLLM(provider, model, agent.apiKey || undefined, temperature)

  // ë„êµ¬ ë°”ì¸ë”©
  const tools = getSuperAgentTools()
  const llmWithTools = llm.bindTools(tools)

  // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
  const role = getAgentRole(agent.capabilities || [])
  const basePersonality = agent.system_prompt || AGENT_ROLE_PROMPTS[role] || AGENT_ROLE_PROMPTS['default']

  // ì •ì²´ì„± ì •ë³´
  let identityStr = ''
  if (agent.identity) {
    const id = agent.identity
    const parts: string[] = ['## ğŸ§  ë‹¹ì‹ ì˜ ì •ì²´ì„±ê³¼ ì„±ê²©']
    if (id.self_summary) parts.push(`\n### ë‚˜ëŠ” ëˆ„êµ¬ì¸ê°€\n${id.self_summary}`)
    if (id.core_values?.length) parts.push(`\n### í•µì‹¬ ê°€ì¹˜\n${id.core_values.map((v: string) => `- ${v}`).join('\n')}`)
    if (id.personality_traits?.length) parts.push(`\n### ì„±ê²© íŠ¹ì„±\n${id.personality_traits.map((t: string) => `- ${t}`).join('\n')}`)
    if (id.communication_style) parts.push(`\n### ì†Œí†µ ìŠ¤íƒ€ì¼\n${id.communication_style}`)
    identityStr = parts.join('\n')
  }

  const coreSystemPrompt = buildDynamicAgentSystemPrompt(
    agent.name,
    basePersonality,
    identityStr,
    '',
    false
  )

  // í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸
  const projectContext = context?.projectPath
    ? `\n## ğŸ“ í˜„ì¬ í”„ë¡œì íŠ¸\n- ê²½ë¡œ: ${context.projectPath}\n`
    : ''

  // ì‚¬ìš©ì ì •ë³´
  const userInfo = context?.userName
    ? `\n## ğŸ‘¤ ëŒ€í™” ìƒëŒ€\n- ì´ë¦„: ${context.userName}${context.userRole ? `\n- ì§ìœ„: ${context.userRole}` : ''}\n`
    : ''

  // ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸
  const workContextStr = context?.workContext
    ? `\n## ğŸ“‹ ì—…ë¬´ ë§¥ë½\n${context.workContext}\n`
    : ''

  // íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ (ìˆëŠ” ê²½ìš°)
  const filesContext = context?.files?.length
    ? `\n## ğŸ“„ ë¡œë“œëœ íŒŒì¼ë“¤\n${context.files.map(f => `- ${f.path}`).join('\n')}\n`
    : ''

  const systemPrompt = `${coreSystemPrompt}

${projectContext}
${userInfo}
${workContextStr}
${filesContext}

## ğŸ§  í•µì‹¬ ì›ì¹™: ì´ˆë³´ìë„ ì“¸ ìˆ˜ ìˆëŠ” AI

ì‚¬ìš©ìëŠ” ì½”ë”©ì„ ëª¨ë¥´ëŠ” ì´ˆë³´ìì…ë‹ˆë‹¤. ëŒ€ì¶© ë§í•´ë„ **ì˜ë„ë¥¼ íŒŒì•…í•´ì„œ ì•Œì•„ì„œ ì‘ì—…**í•˜ì„¸ìš”.

### ì‚¬ìš©ìê°€ ì´ë ‡ê²Œ ë§í•˜ë©´:
- "ê²Œì„ ë§Œë“¤ì–´ì¤˜" â†’ ì–´ë–¤ ê²Œì„ì¸ì§€ ì¶”ë¡ í•´ì„œ ë°”ë¡œ ì½”ë“œ ì‘ì„±
- "ë­”ê°€ ë©‹ì§„ ê±°" â†’ ì ì ˆí•œ í”„ë¡œì íŠ¸ ì„ íƒí•´ì„œ êµ¬í˜„
- "ì €ë²ˆì— í•˜ë˜ ê±°" â†’ ì»¨í…ìŠ¤íŠ¸ì—ì„œ íŒŒì•…í•´ì„œ ì´ì–´ì„œ ì‘ì—…
- "ì´ê±° ê³ ì³" â†’ ë¬´ì—‡ì´ ë¬¸ì œì¸ì§€ ë¶„ì„í•˜ê³  ìˆ˜ì •
- "ë” ì¢‹ê²Œ" â†’ ê°œì„ ì  ì°¾ì•„ì„œ ë¦¬íŒ©í† ë§

### ë‹¹ì‹ ì´ í•´ì•¼ í•  ê²ƒ:
1. **ì˜ë„ íŒŒì•…**: ëª¨í˜¸í•œ ìš”ì²­ì—ì„œ êµ¬ì²´ì  ì‘ì—… ì¶”ì¶œ
2. **ê³„íš ìˆ˜ë¦½**: í•„ìš”í•œ íŒŒì¼, êµ¬ì¡°, ê¸°ìˆ  ìŠ¤íƒ ê²°ì •
3. **ì¦‰ì‹œ ì‹¤í–‰**: ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ë°”ë¡œ ë§Œë“¤ê¸°
4. **ê²°ê³¼ ë³´ê³ **: ë­˜ ë§Œë“¤ì—ˆëŠ”ì§€ ê°„ë‹¨íˆ ì„¤ëª…

## ğŸ› ï¸ ë„êµ¬ (ë°˜ë“œì‹œ ì‚¬ìš©!)

### ì½”ë“œ/íŒŒì¼ ì‘ì—…
- **create_file_with_node** - â­ ì½”ë“œ íŒŒì¼ ìƒì„± + ë‰´ëŸ°ë§µ ë…¸ë“œ (ê°€ì¥ ë§ì´ ì”€!)
- **edit_file** - ê¸°ì¡´ íŒŒì¼ ìˆ˜ì •
- **read_file** - íŒŒì¼ ë‚´ìš© í™•ì¸
- **get_file_structure** - í”„ë¡œì íŠ¸ êµ¬ì¡° íŒŒì•…

### ë‰´ëŸ°ë§µ ë…¸ë“œ
- **create_node** - ë…¸íŠ¸, ë‹¤ì´ì–´ê·¸ë¨, ë¬¸ì„œ ë“± ë…¸ë“œ ìƒì„±
- **update_node** / **delete_node** - ë…¸ë“œ ìˆ˜ì •/ì‚­ì œ
- **create_edge** - ë…¸ë“œ ì—°ê²°

### ê¸°íƒ€
- **run_terminal** - npm install, git ë“± ëª…ë ¹ ì‹¤í–‰
- **web_search** - ëª¨ë¥´ëŠ” ê±° ê²€ìƒ‰

## ğŸš¨ ì ˆëŒ€ ê·œì¹™

### âŒ í•˜ì§€ ë§ˆ:
- "ì–´ë–¤ ê²Œì„ì„ ë§Œë“¤ê¹Œìš”?" ê°™ì€ ì—­ì§ˆë¬¸
- "~í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤" ê°™ì€ ì„¤ëª…ë§Œ
- "draw.io ì‚¬ìš©í•˜ì„¸ìš”" ê°™ì€ ì™¸ë¶€ ë„êµ¬ ì¶”ì²œ
- ì½”ë“œ ì—†ì´ ì„¤ëª…ë§Œ í•˜ê¸°

### âœ… ë¬´ì¡°ê±´ í•´:
- **ì¼ë‹¨ ë§Œë“¤ì–´!** ì§ˆë¬¸í•˜ì§€ ë§ê³  ê°€ì¥ ì ì ˆí•œ ê±¸ ì„ íƒí•´ì„œ êµ¬í˜„
- ëª¨ë“  ì½”ë“œëŠ” **create_file_with_node**ë¡œ íŒŒì¼+ë…¸ë“œ ìƒì„±
- ëª¨ë“  ë¬¸ì„œ/ë…¸íŠ¸ëŠ” **create_node**ë¡œ ë‰´ëŸ°ë§µì— ì¶”ê°€

## ì˜ˆì‹œ

ì‚¬ìš©ì: "ê²Œì„ ë§Œë“¤ì–´"
â†’ ìƒê°: ê°„ë‹¨í•œ ê²Œì„... ë²½ëŒê¹¨ê¸°ë‚˜ ìŠ¤ë„¤ì´í¬ê°€ ì ë‹¹
â†’ í–‰ë™: create_file_with_nodeë¡œ game.html ìƒì„± (Canvas ê¸°ë°˜ ë²½ëŒê¹¨ê¸°)

ì‚¬ìš©ì: "ì´ê±° ë­”ê°€ ì´ìƒí•´"
â†’ ìƒê°: í˜„ì¬ í”„ë¡œì íŠ¸ íŒŒì¼ í™•ì¸ í•„ìš”
â†’ í–‰ë™: get_file_structureë¡œ êµ¬ì¡° íŒŒì•… â†’ read_fileë¡œ ì½”ë“œ í™•ì¸ â†’ edit_fileë¡œ ìˆ˜ì •

ì‚¬ìš©ì: "ë¬¸ì„œ ì •ë¦¬í•´ì¤˜"
â†’ ìƒê°: í”„ë¡œì íŠ¸ êµ¬ì¡°ë¥¼ ë…¸íŠ¸ë¡œ ì •ë¦¬
â†’ í–‰ë™: create_node(type="note")ë¡œ ë¬¸ì„œ ë…¸ë“œ ìƒì„±

**ë„ˆëŠ” ì‹¤í–‰í•˜ëŠ” AIë‹¤. ë§ë§Œ í•˜ëŠ” AI ì•„ë‹ˆë‹¤. ë„êµ¬ ì¨ì„œ ë§Œë“¤ì–´!**
`

  // ë©”ì‹œì§€ ë°°ì—´ êµ¬ì„±
  const messages: (SystemMessage | HumanMessage | AIMessage | ToolMessage)[] = [
    new SystemMessage(systemPrompt),
  ]

  // ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¶”ê°€
  for (const msg of chatHistory.slice(-20)) {
    if (msg.role === 'user') {
      messages.push(new HumanMessage(msg.content))
    } else if (msg.role === 'assistant') {
      messages.push(new AIMessage(msg.content))
    } else if (msg.role === 'tool' && msg.toolCallId) {
      messages.push(new ToolMessage({ content: msg.content, tool_call_id: msg.toolCallId }))
    }
  }

  // í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
  messages.push(new HumanMessage(userMessage))

  // Tool Calling ë£¨í”„
  const actions: ToolAction[] = []
  const toolsUsed: string[] = []
  let finalResponse = ''
  let iterations = 0
  const maxIterations = 5  // ë¬´í•œ ë£¨í”„ ë°©ì§€

  try {
    while (iterations < maxIterations) {
      iterations++
      console.log(`[SuperAgent] Iteration ${iterations}`)

      // LLM í˜¸ì¶œ
      const response = await llmWithTools.invoke(messages)

      // Tool Call í™•ì¸
      const toolCalls = response.tool_calls || []

      if (toolCalls.length === 0) {
        // Tool Call ì—†ìŒ - ìµœì¢… ì‘ë‹µ
        finalResponse = typeof response.content === 'string'
          ? response.content
          : JSON.stringify(response.content)
        break
      }

      // Tool Call ìˆìŒ - ë„êµ¬ ì‹¤í–‰
      messages.push(new AIMessage({
        content: response.content || '',
        tool_calls: toolCalls.map(tc => ({
          id: tc.id || `tool_${Date.now()}`,
          name: tc.name,
          args: tc.args,
        })),
      }))

      for (const toolCall of toolCalls) {
        const toolName = toolCall.name
        const toolArgs = toolCall.args || {}
        const toolId = toolCall.id || `tool_${Date.now()}`

        console.log(`[SuperAgent] Tool call: ${toolName}`, toolArgs)
        toolsUsed.push(toolName)

        // ë„êµ¬ ì°¾ê¸° ë° ì‹¤í–‰
        const tool = tools.find(t => t.name === toolName)
        if (!tool) {
          messages.push(new ToolMessage({
            content: JSON.stringify({ success: false, error: `ë„êµ¬ "${toolName}"ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.` }),
            tool_call_id: toolId,
          }))
          continue
        }

        try {
          // ë„êµ¬ ì‹¤í–‰
          const result = await tool.invoke(toolArgs)
          const parsedResult = typeof result === 'string' ? JSON.parse(result) : result

          // ì•¡ì…˜ ìˆ˜ì§‘ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‹¤í–‰í•  ê²ƒë“¤)
          if (parsedResult.action) {
            actions.push(parsedResult.action)
          }

          messages.push(new ToolMessage({
            content: result,
            tool_call_id: toolId,
          }))
        } catch (error: any) {
          messages.push(new ToolMessage({
            content: JSON.stringify({ success: false, error: error.message }),
            tool_call_id: toolId,
          }))
        }
      }
    }

    // ì‘ë‹µ ì •ë¦¬
    let cleanResponse = finalResponse
    cleanResponse = cleanResponse.replace(/<think>[\s\S]*?<\/think>\s*/g, '')
    cleanResponse = cleanResponse.replace(/<thinking>[\s\S]*?<\/thinking>\s*/g, '')

    return {
      message: cleanResponse.trim() || 'ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.',
      actions,
      toolsUsed,
    }
  } catch (error: any) {
    console.error('[SuperAgent] Error:', error)
    return {
      message: `ì£„ì†¡í•´ìš”, ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”: ${error.message}`,
      actions: [],
      toolsUsed,
    }
  }
}

// ============================================
// ì•¡ì…˜ ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬
// ============================================
export interface ActionExecutionResult {
  action: ToolAction
  success: boolean
  result?: unknown
  error?: string
}

export function formatActionResults(results: ActionExecutionResult[]): string {
  if (results.length === 0) return ''

  const lines: string[] = ['## ì‹¤í–‰ ê²°ê³¼']

  for (const r of results) {
    const status = r.success ? 'âœ…' : 'âŒ'
    const type = r.action.type

    switch (type) {
      case 'create_project':
        lines.push(`${status} í”„ë¡œì íŠ¸ ìƒì„±: ${r.action.data.name}`)
        break
      case 'write_file':
      case 'edit_file':
        lines.push(`${status} íŒŒì¼ ìˆ˜ì •: ${r.action.data.path}`)
        break
      case 'terminal_cmd':
        lines.push(`${status} ëª…ë ¹ ì‹¤í–‰: ${r.action.data.command}`)
        if (r.result) lines.push(`   ê²°ê³¼: ${String(r.result).slice(0, 200)}`)
        break
      case 'create_task':
        lines.push(`${status} íƒœìŠ¤í¬ ìƒì„±: ${r.action.data.title}`)
        break
      default:
        lines.push(`${status} ${type}: ${JSON.stringify(r.action.data).slice(0, 100)}`)
    }

    if (r.error) {
      lines.push(`   ì˜¤ë¥˜: ${r.error}`)
    }
  }

  return lines.join('\n')
}
